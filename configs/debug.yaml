defaults:
  - _self_
  - model: small

ngpus: 1
tokens: 50257 # comes from gpt2 tokenizer

training:
  batch_size: 8
  accum: 1
  n_iters: 10
  snapshot_freq: 5
  log_freq: 1
  eval_freq: 5
  snapshot_freq_for_preemption: 1000
  weight: standard
  snapshot_sampling: True
  ema: 0.9999

data:
  train: wikitext103
  valid: wikitext103
  cache_dir: data

graph:
  type: absorb
  file: data
  report_all: True

noise:
  type: loglinear
  sigma_min: 1e-4
  sigma_max: 20

sampling:
  predictor: euler
  steps: 128
  noise_removal: True

eval:
  batch_size: 8
  perplexity: True
  perplexity_batch_size: 32

optim:
  weight_decay: 0
  optimizer: AdamW
  lr: 3e-4
  beta1: 0.9
  beta2: 0.999
  eps: 1e-8
  warmup: 2500
  grad_clip: 1.


hydra:
  run:
    dir: exp_local/${data.train}/${now:%Y.%m.%d}/${now:%H%M%S}
  sweep:
    dir: exp/${data.train}/${now:%Y.%m.%d}/${now:%H%M%S}
    subdir: ${hydra.job.num}

